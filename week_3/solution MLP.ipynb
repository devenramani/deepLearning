{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ee436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the digits dataset \n",
    "digits = load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "print(\"data shape: \",x.shape)\n",
    "print(\"class shape: \",y.shape)\n",
    "\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_val = x[N_train:,:]\n",
    "y_val = y[N_train:]\n",
    "\n",
    "# Add the bias term\n",
    "x_train = np.hstack((x_train, np.ones((x_train.shape[0], 1))))\n",
    "x_val = np.hstack((x_val, np.ones((x_val.shape[0], 1))))\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(x_train, dtype=torch.float)\n",
    "Y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val = torch.tensor(x_val, dtype=torch.float)\n",
    "Y_val = torch.tensor(y_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset object to support batch training\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features             \n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d421b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleMLP(nn.Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs,n_classes,hidden_size):\n",
    "        super(simpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, hidden_size)\n",
    "        self.output  = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)        \n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x=self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa06214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class complexMLP(nn.Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs,n_classes,hidden_size=None):\n",
    "        super(complexMLP, self).__init__()\n",
    "        hidden_size=100\n",
    "        dropout=0.2\n",
    "        # Add the first fully connected layer with 100 hidden units with batch norm, dropout of 0.2\n",
    "        self.fc1 = nn.Linear(n_inputs, hidden_size)\n",
    "        self.batchnormfc1=nn.BatchNorm1d(hidden_size)  \n",
    "        \n",
    "        self.dropoutfc1 = nn.Dropout(dropout)\n",
    "        hidden_size_2=32\n",
    "        # Add the second fully connected layer with 32 hidden units with batch norm, dropout of 0.2\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size_2)\n",
    "        self.batchnormfc2=nn.BatchNorm1d(hidden_size_2)  \n",
    "        self.dropoutfc2 = nn.Dropout(dropout)\n",
    "       \n",
    "        self.output  = nn.Linear(hidden_size_2, 10)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.fc1(X)\n",
    "        X=self.batchnormfc1(X)\n",
    "        X = torch.nn.functional.relu(X)\n",
    "        X = self.dropoutfc1(X)\n",
    "       \n",
    "        X = self.fc2(X)\n",
    "        X=self.batchnormfc2(X)\n",
    "        X = torch.nn.functional.relu(X)\n",
    "        X = self.dropoutfc2(X)\n",
    "        \n",
    "        X=self.output(X)\n",
    "       \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model,data):\n",
    "    return model(data).numpy().argmax(axis=1)  \n",
    "\n",
    "def compute_accuracy(predictions,ground_truth):\n",
    "    # Find the prediction (as the classes with highest probabilities)\n",
    "    return (predictions == ground_truth.numpy()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d571816",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In PyTorch, we could implement regularization pretty easily by adding a term to\n",
    "the loss. After computing the loss, whatever the loss function is, we can iterate the\n",
    "parameters of the model, sum their respective square (for L2) or abs (for L1), and\n",
    "backpropagate.\n",
    "\"\"\"\n",
    "def l2_regularizer(model,l2_lambda):\n",
    "    #This is for step 9: implement the l2 regularization \n",
    "    l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "    return l2_lambda * l2_norm\n",
    "def l1_regularizer(model,l1_lambda):\n",
    "    #This is for step 9: implement the l2 regularization \n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())    \n",
    "    return l1_lambda * l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(func,optimizer,learning_rate,num_epochs,hidden_size=None,regularization=None):\n",
    "    model = func(X_train.shape[1],len(list(set(Y_train.numpy()))),hidden_size)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer(model.parameters(),lr=learning_rate)\n",
    "    batch_size = 16\n",
    "\n",
    "    # enable batching of training data\n",
    "    train_dataset = Dataset(X_train, Y_train)\n",
    "    dataloader = DataLoader(train_dataset,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True)\n",
    "\n",
    "    # keep the accuracy values for each training step\n",
    "    val_accs = np.zeros(num_epochs)\n",
    "\n",
    "    for i_epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i_batch, (X_batch, Y_batch) in enumerate(dataloader):\n",
    "            model.zero_grad()  # reset model gradients\n",
    "            output = model(X_batch)  # conduct forward pass  \n",
    "\n",
    "            loss=criterion(output, Y_batch) \n",
    "            if regularization ==\"l2\":\n",
    "                loss+=l2_regularizer(model,0.001)\n",
    "            if regularization ==\"l1\":\n",
    "                loss+=l1_regularizer(model,0.001)\n",
    "\n",
    "            loss.backward()  # backpropogate loss to calculate gradients\n",
    "            optimizer.step()  # update model weights\n",
    "\n",
    "        with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
    "\n",
    "            model.eval()        \n",
    "            pred_train = get_predictions(model,X_train)        \n",
    "            train_acc = compute_accuracy(pred_train, Y_train)\n",
    "            #print(\"Training accuracy: {}\".format(train_acc))       \n",
    "            pred_val = get_predictions(model,X_val)        \n",
    "            val_acc = compute_accuracy(pred_val, Y_val)\n",
    "            val_accs[i_epoch]=val_acc\n",
    "            #print(\"Validation accuracy: {}\".format(val_acc))\n",
    "    \n",
    "    return val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c438d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam\n",
    "val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=32)\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e664830",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam\n",
    "lrs=[1e-1,1e-2, 1e-3, 1e-4, 1e-5]\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "for lr in lrs:\n",
    "    val_acc=run_model(simpleMLP,optimizer,learning_rate=lr,num_epochs=50,hidden_size=32)\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(val_acc,color=color)\n",
    "    \n",
    "    \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "\n",
    "labels=[]\n",
    "for lr in lrs:\n",
    "    labels.append('lr_' + str(lr))\n",
    "\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a73749",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam\n",
    "hss=[16,32,64,128,256,512]\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "for hs in hss:\n",
    "    val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=hs)\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(val_acc,color=color) \n",
    "    \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "\n",
    "labels=[]\n",
    "for hs in hss:\n",
    "    labels.append('bs_' + str(hs))\n",
    "\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers=[torch.optim.Adam,torch.optim.SGD,torch.optim.Adagrad,torch.optim.Adadelta,torch.optim.RMSprop]\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "for opt in optimizers:\n",
    "    val_acc=run_model(simpleMLP,opt,learning_rate=0.001,num_epochs=50,hidden_size=256)\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(val_acc,color=color) \n",
    "    \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "\n",
    "labels=[]\n",
    "for opt in optimizers:\n",
    "    labels.append(str(opt))\n",
    "\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7eb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For step 9\n",
    "val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=32,regularization=\"l2\")\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1002a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For step 9\n",
    "val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=32,regularization=\"l1\")\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For step 10\n",
    "val_acc=run_model(complexMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=None)\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403316f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
