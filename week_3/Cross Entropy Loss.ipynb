{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7719fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(vec):\n",
    "    #implement the log sum exp manually using the formula defined at the exercise (Equation 4)\n",
    "    \n",
    "    return lse\n",
    "\n",
    "def loss_plain_softmax(x1,index):\n",
    "    #STEP 4: return the plain softmax as this has been defined at the exercise (Equation 1)\n",
    "    return sftm\n",
    "\n",
    "def loss_logsumexp(x1,index):\n",
    "    #STEP 5: return the softmax without the trick as this has been defined at the exercise (Equation 3)\n",
    "    return sftm\n",
    "\n",
    "def loss_logsumexp_stable(x1,index):\n",
    "    #STEP 6: return the softmax with the trick as this has been defined at the exercise using the torch function logsumexp\n",
    "    return sftm\n",
    "\n",
    "def loss_logsumexp_stable_own(x1,index):\n",
    "    #STEP 7: return the softmax with the trick as this has been defined at the exercise using your own implementation of logsumexp (defined above)\n",
    "    return sftm\n",
    "\n",
    "def compute_crossentropyloss_manual(func,x,y0):\n",
    "    \"\"\"\n",
    "    x is the vector with shape (batch_size,C)\n",
    "    y0 shape is the same (batch_size), whose entries are integers from 0 to C-1\n",
    "    \"\"\"\n",
    "    loss = 0.\n",
    "    n_batch, n_class = x.shape\n",
    "    for x1,y1 in zip(x,y0):        \n",
    "        loss = loss + func(x1,y1)\n",
    "    loss = - loss/n_batch\n",
    "    return loss\n",
    "\n",
    "def compute_percentage_loss(loss_name,uni_s,uni_e):\n",
    "    #loss_name: is the name of the manually defined function, e.g., loss_plain_softmax that you will call to compute the cross entropy loss \n",
    "    #uni_s: the uniform ditribution takes values between uni_s\n",
    "    #uni_e: and uni_e\n",
    "    torch.manual_seed(0)\n",
    "    precision = 3\n",
    "\n",
    "    batch_size=10\n",
    "    C = 15\n",
    "    N_iter = 100\n",
    "    n_correct_CE = 0\n",
    "\n",
    "    criterion2 = #STEP 2: define here the criterion for cross-entropy loss in Pytorch\n",
    "    for idx in range(N_iter):\n",
    "        #STEP 1: initialize a distribution of size batch_size,C that is uniform \n",
    "        #from (uni_s,uni_e) with float values\n",
    "        x = torch.rand(size=(batch_size,C)).uniform_(uni_s,uni_e).to(torch.float)   \n",
    "        #STEP 1: initialize a vector of size batch size that takes random int values between 0 and C\n",
    "        y0 = torch.randint(0,C,size=(batch_size,))\n",
    "\n",
    "        CEloss = #STEP 2: #compute the cross-entropy loss using  Pytorch \n",
    "        manual_CEloss = #STEP 3: compute the cross entropy loss manually - call function compute_crossentropyloss_manual\n",
    "        if idx==0:\n",
    "            print('CrossEntropyLoss:')\n",
    "            print('module:%s'%(str(CEloss)))\n",
    "            print('manual:%s'%(str(manual_CEloss)))\n",
    "\n",
    "\n",
    "        CE_loss_check = # STEP 8: return True in the case that the precision between the Pytorch computed loss and the \n",
    "                        #manually computed loss is above the precision of 3 digits defined above\n",
    "        if CE_loss_check: n_correct_CE+=1\n",
    "    \n",
    "    percentage # STEP 8:compute the percentage of the CELoss correctly computed in the number of iterations\n",
    "    print('percentage CELoss correctly computed :%s'%(percentage)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e12d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: 9 Compute the percentage loss when the uniform distribution takes values \n",
    "#between 0 and 1 for all the manually computed versions of the cross-entropy loss (compared to the Pytorch cross-entropy loss)\n",
    "start=0\n",
    "end=1\n",
    "compute_percentage_loss(loss_plain_softmax,start,end)\n",
    "compute_percentage_loss(loss_logsumexp,start,end)\n",
    "compute_percentage_loss(loss_logsumexp_stable,start,end)\n",
    "compute_percentage_loss(loss_logsumexp_stable_own,start,end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7336f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: 9 Compute the percentage loss when the uniform distribution takes values \n",
    "#between 0 and 100 for all the manually computed versions of the cross-entropy loss (compared to the Pytorch cross-entropy loss)\n",
    "start=0\n",
    "end=100\n",
    "compute_percentage_loss(loss_plain_softmax,start,end)\n",
    "compute_percentage_loss(loss_logsumexp,start,end)\n",
    "compute_percentage_loss(loss_logsumexp_stable,start,end)\n",
    "compute_percentage_loss(loss_logsumexp_stable_own,start,end)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
