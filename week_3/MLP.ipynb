{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ee436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the digits dataset \n",
    "digits = load_digits()\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "print(\"data shape: \",x.shape)\n",
    "print(\"class shape: \",y.shape)\n",
    "\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_val = x[N_train:,:]\n",
    "y_val = y[N_train:]\n",
    "\n",
    "# Add the bias term\n",
    "x_train = np.hstack((x_train, np.ones((x_train.shape[0], 1))))\n",
    "x_val = np.hstack((x_val, np.ones((x_val.shape[0], 1))))\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Add code to transform the data into Pytorch tensors. Hint! The features (i.e. images) should be of type float and \n",
    "labels of type long \n",
    "'''\n",
    "X_train = # TODO: add code\n",
    "Y_train = # TODO: add code\n",
    "X_val = # TODO: add code\n",
    "Y_val = # TODO: add code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset object to support batch training\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        # TODO: load your features and the labels in this class\n",
    "        self.features = # TODO: add code\n",
    "        self.labels = # TODO: add code\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleMLP(nn.Module):\n",
    "    # TODO: define model elements\n",
    "    #create a neural network with two fully connected layers. You should use the Pytorch Linear module\n",
    "    #Input and output of the network is straighforward and the hidden layer size is given by the parameter hidden_size.\n",
    "    #In between the two Linear layers you should use a relu activation\n",
    "    def __init__(self, n_inputs,n_classes,hidden_size):\n",
    "        super(simpleMLP, self).__init__()\n",
    "        \n",
    "\n",
    "    #TODO: forward propagate input\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class complexMLP(nn.Module):\n",
    "    # TODO: define model elements\n",
    "    def __init__(self, n_inputs,n_classes,hidden_size=None):\n",
    "        super(complexMLP, self).__init__()\n",
    "        hidden_size=100\n",
    "        dropout=0.2\n",
    "        # TODO: Add the first fully connected layer with 100 hidden units with batch norm, dropout of 0.2\n",
    "\n",
    "        hidden_size_2=32\n",
    "        # TODO: Add the second fully connected layer with 32 hidden units with batch norm, dropout of 0.2\n",
    "        \n",
    "        #TODO: project to the number of classes using a third fully connected layer\n",
    "        \n",
    "\n",
    "    # TODO: forward propagate input\n",
    "    def forward(self, x):\n",
    "        \n",
    "               \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model,data):\n",
    "    #TODO: return the predictions of the model    \n",
    "    # Find the prediction (as the classes with highest values in the vector)\n",
    "    predictions = # TODO: add code\n",
    "    return predictions  \n",
    "def compute_accuracy(predictions,ground_truth):\n",
    "    #TODO: return the accuracy evaluation metric\n",
    "    accuracy =  # TODO: add code\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d934f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In PyTorch, we could implement regularization pretty easily by adding a term to\n",
    "the loss. After computing the loss, whatever the loss function is, we can iterate the\n",
    "parameters of the model, sum their respective square (for L2) or abs (for L1), and\n",
    "backpropagate.\n",
    "Hint! You can find the parameters of the model in model.parameters()\n",
    "\"\"\"\n",
    "def l2_regularizer(model,l2_lambda):\n",
    "    #TODO: This is for step 9: implement the l2 regularization \n",
    "    l2_norm = # TODO: add code\n",
    "    return l2_lambda * l2_norm\n",
    "\n",
    "def l1_regularizer(model,l1_lambda):\n",
    "    #TODO: This is for step 9: implement the l2 regularization \n",
    "    l1_norm = # TODO: add code\n",
    "    return l1_lambda * l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(func,optimizer,learning_rate,num_epochs,hidden_size=None,regularization=None):\n",
    "    \n",
    "    model = func() # TODO: add the required arguments\n",
    "    criterion = # TODO: use the Cross Entropy Loss\n",
    "    optimizer = optimizer(model.parameters(),lr=learning_rate)\n",
    "    batch_size = 16\n",
    "\n",
    "    # enable batching of training data\n",
    "    train_dataset = Dataset(X_train, Y_train) #TODO: Fill in also the Dataset class above\n",
    "    dataloader = #TODO: Use the Pytorch dataloader to enable batching of the data\n",
    "    \n",
    "    \n",
    "    val_accs = np.zeros(num_epochs)\n",
    "\n",
    "    for i_epoch in range(num_epochs):\n",
    "        #TODO: enable the training mode in the model \n",
    "        \n",
    "        for i_batch, (X_batch, Y_batch) in enumerate(dataloader):\n",
    "            # TODO: reset model gradients\n",
    "            prediction = # TODO: conduct a forward pass and get the scores for the classes\n",
    "            loss = # TODO: compute the loss\n",
    "            # TODO: for Step 9 add the regularization term to the loss\n",
    "            if regularization ==\"l2\":\n",
    "                loss += #TODO: add code\n",
    "            if regularization ==\"l1\":\n",
    "                loss += #TODO: add code\n",
    "            # TODO: backpropogate loss to calculate gradients\n",
    "            # TODO: update model weights\n",
    "\n",
    "        with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
    "\n",
    "            #TODO: enable the evaluation mode in the model    \n",
    "            #TODO: fill in the following two functions\n",
    "            pred_train = get_predictions(model,X_train) #get the predictions of your model on the training set   \n",
    "            train_acc = compute_accuracy(pred_train, Y_train) #calculate the accuracy of your model on the training set. \n",
    "                                                              #This can used for debugging purposes (i.e., to see that \n",
    "                                                              #your model can memorize the training set)  \n",
    "            \n",
    "            print(\"Training accuracy: {}\".format(train_acc))       \n",
    "            pred_val = get_predictions(model,X_val)        #get the predictions of your model on the val set   \n",
    "            val_acc = compute_accuracy(pred_val, Y_val)    #calculate the accuracy of your model on the val set. \n",
    "            val_accs[i_epoch]=val_acc\n",
    "            print(\"Validation accuracy: {}\".format(val_acc))\n",
    "    \n",
    "    return val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a42044",
   "metadata": {},
   "source": [
    "## Run the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will use Adam as an optimizer, a learning rate of 0.001, and a hidden size of 32. You will train the model for 50 epochs. \n",
    "optimizer=torch.optim.Adam\n",
    "val_acc=run_model(simpleMLP,...)# TODO: add the required arguments\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0578af",
   "metadata": {},
   "source": [
    "### Run the model with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e664830",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam\n",
    "lrs=[]# TODO: add the required values\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "for lr in lrs:\n",
    "    val_acc=run_model(simpleMLP,optimizer,learning_rate=lr,num_epochs=50,hidden_size=32)\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(val_acc,color=color)\n",
    "    \n",
    "    \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "\n",
    "labels=[]\n",
    "for lr in lrs:\n",
    "    labels.append('lr_' + str(lr))\n",
    "\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ff46e",
   "metadata": {},
   "source": [
    "### Run the model with different neural network hidden sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a73749",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam\n",
    "hss=[]# TODO: add the required values\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "for hs in hss:\n",
    "    val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=hs)\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(val_acc,color=color) \n",
    "    \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "\n",
    "labels=[]\n",
    "for hs in hss:\n",
    "    labels.append('bs_' + str(hs))\n",
    "\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526bd09",
   "metadata": {},
   "source": [
    "### Run the model with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers=[torch.optim.Adam,torch.optim.SGD,torch.optim.Adagrad,torch.optim.Adadelta,torch.optim.RMSprop]\n",
    "fig = plt.subplot(111)\n",
    "ax = plt.gca()\n",
    "for opt in optimizers:\n",
    "    val_acc=run_model(simpleMLP,opt,learning_rate=0.001,num_epochs=50,hidden_size=256)\n",
    "    color = next(ax._get_lines.prop_cycler)['color']\n",
    "    plt.plot(val_acc,color=color) \n",
    "    \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "\n",
    "labels=[]\n",
    "for opt in optimizers:\n",
    "    labels.append(str(opt))\n",
    "\n",
    "plt.legend(labels, loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20faee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9\n",
    "val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=32,regularization=\"l2\")\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9\n",
    "val_acc=run_model(simpleMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=32,regularization=\"l1\")\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c1f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 10\n",
    "val_acc=run_model(complexMLP,optimizer,learning_rate=0.001,num_epochs=50,hidden_size=None)\n",
    "plt.plot(val_acc)  \n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Val Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
