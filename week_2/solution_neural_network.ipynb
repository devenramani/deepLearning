{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (901, 64)\n",
      "class shape:  (901,)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load digits dataset with 5 classes. The dataset has 10 classes in total. \n",
    "# You can change the amount of data as you like.\n",
    "num_classes = 5\n",
    "digits = load_digits(n_class=num_classes)\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "print(\"data shape: \",x.shape)\n",
    "print(\"class shape: \",y.shape)\n",
    "\n",
    "N_train = int(0.8 * x.shape[0])\n",
    "x_train = x[:N_train,:]\n",
    "y_train = y[:N_train]\n",
    "x_test = x[N_train:,:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "# Add the bias term\n",
    "x_train = np.hstack((x_train, np.ones((x_train.shape[0], 1)))) # Your code\n",
    "x_test = np.hstack((x_test, np.ones((x_test.shape[0], 1)))) # Your code\n",
    "\n",
    "# Convert labels to one-hot vector\n",
    "y_train_onehot = np.zeros((len(y_train), num_classes)) # Your code here\n",
    "y_train_onehot[np.arange(y_train_onehot.shape[0]), y_train] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a sigmoid activation with one-hot labels for classification, the network outputs a probability for each possible class. This is a clear advantage over using the original form of labels. For example, when the network predicts a sample as number 1 and number 3 with 50% and 40% probabilities, respectively, we know that the sample could be a number 3, but it will be more likely to be a number 1. If we don't use one-hot encoding, the output would then likely be in the range of number 2, which would be completely wrong.\n",
    "\n",
    "Check whether your one-hot conversion above is correct or not by the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])\n",
    "print(y_train_onehot[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Forward computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most deep learning frameworks provide a list of basic functions as building blocks, such as fully_connected, sigmoid, relu... so that you can stack them sequentially as layers to build your own neural networks. In this exercise, we will see implement the fully connected layer and the sigmoid activation function. In each function, we will return the result and the cache the input for backward computation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the probability of y=1\n",
    "def sigmoid(x):\n",
    "    cache = x\n",
    "    result = 1.0/(1 + np.exp(-x))\n",
    "    return cache, result\n",
    "\n",
    "def fully_connected(x, theta):\n",
    "    cache = (x, theta)\n",
    "    result = x.dot(theta) # Your code\n",
    "    return cache, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having our building blocks, we can start stacking layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward(x, theta_matrices):\n",
    "    '''\n",
    "    x: feature vector\n",
    "    theta_matrices: The list contains all theta. The first element is the theta (matrix) of the input layer and the first hidden\n",
    "    layer, the second one is the theta of the fist hidden layer and the second hidden layer, and so on\n",
    "    \n",
    "    In this exercise, our network architecture will be: \n",
    "    input -> fully_connected -> sigmoid -> fully_connected -> sigmoid -> output\n",
    "    You don't need to use regularization in this exercise\n",
    "    '''\n",
    "    result = x\n",
    "    cache = dict() \n",
    "    for i, theta in enumerate(theta_matrices) :\n",
    "        ## Your code here, should be a result of a fully_connected layer then a sigmoid activation.\n",
    "        # Store the result of each computation in cache, for doing backprop later.\n",
    "        # For this exercise, cache should have four items with keys: fc0, sigmoid0, fc1, sigmoid1\n",
    "        cache['fc'+ str(i)], result = fully_connected(result, theta)\n",
    "        cache['sigmoid'+ str(i)], result = sigmoid(result)\n",
    "    return cache, result\n",
    "\n",
    "def compute_cost(outputs, labels):\n",
    "    '''mean square error'''\n",
    "    result = np.mean((outputs - labels) ** 2)/2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize theta and check the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass returns correct shape\n",
      "0.31845898157997143\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 100\n",
    "theta0 = np.random.normal(loc=0., scale=0.5, size=(n_features+1, num_hidden+1)) # + 1 for bias term\n",
    "theta1 = np.random.normal(loc=0., scale=0.5, size=(num_hidden+1, num_classes))\n",
    "theta_matrices = [theta0, theta1]\n",
    "cache, initial_outputs = compute_forward(x_train, theta_matrices)\n",
    "assert initial_outputs.shape == y_train_onehot.shape, 'forward pass returns wrong shape'\n",
    "print('forward pass returns correct shape')\n",
    "initial_cost = compute_cost(initial_outputs, y_train_onehot)\n",
    "print(initial_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to forward pass, calculating backward gradient using backpropagation is just like stacking several layers of gradient together. To do so, we first need to calculate the gradient of each of our building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(cache, result):\n",
    "    x = cache\n",
    "    sigmoid_grad = sigmoid(x)[1] * (1 - sigmoid(x)[1])\n",
    "    return sigmoid_grad * result\n",
    "    \n",
    "def fc_backward(cache, result):\n",
    "    x, theta = cache\n",
    "    theta_grad = x.T.dot(result)\n",
    "    x_grad = result.dot(theta.T)\n",
    "    return x_grad, theta_grad\n",
    "\n",
    "def cost_backward(outputs, labels):\n",
    "    return outputs - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backprop(x, theta_matrices, cache, outputs, labels):\n",
    "    '''\n",
    "    return gradients for theta_matrices\n",
    "    '''\n",
    "    theta_grad = {} # should include two key theta0 and theta1 for this exercise\n",
    "    grad = cost_backward(outputs, labels)\n",
    "    for i, theta in enumerate(theta_matrices[::-1]):\n",
    "        layer = len(theta_matrices) - i - 1\n",
    "        sigmoid_cache = cache['sigmoid' + str(layer)]\n",
    "        grad = sigmoid_backward(sigmoid_cache, grad)\n",
    "        fc_cache = cache['fc' + str(layer)]\n",
    "        grad, theta_grad['theta' + str(layer)] = fc_backward(fc_cache, grad)\n",
    "    return theta_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if compute_backprop returns the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_grad = compute_backprop(x_train, theta_matrices, cache, initial_outputs, y_train_onehot)\n",
    "dtheta0 = theta_grad['theta0']\n",
    "assert dtheta0.shape == theta0.shape, 'backprop returns wrong shape for theta 0'\n",
    "dtheta1 = theta_grad['theta1']\n",
    "assert dtheta1.shape == theta1.shape, 'backprop returns wrong shape for theta 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have both forward and backward computation, use batch gradient descent to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training cost: 0.003611\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = 0.001 # learning rate\n",
    "N_iterations = 200\n",
    "J = np.zeros(N_iterations)\n",
    "\n",
    "for i in range(N_iterations):\n",
    "    ## \n",
    "    cache, outputs = compute_forward(x_train, theta_matrices)\n",
    "    J[i] = compute_cost(outputs, y_train_onehot)\n",
    "    theta_grad = compute_backprop(x_train, theta_matrices, cache, outputs, y_train_onehot)\n",
    "    for layer in range(len(theta_matrices)):\n",
    "        theta_matrices[layer] -= alpha * theta_grad['theta'+str(layer)]\n",
    "\n",
    "# calculate the loss on the whole training set \n",
    "J_train = compute_cost(compute_forward(x_train, theta_matrices)[1], y_train_onehot)\n",
    "print('training cost: %f' %J_train)\n",
    "\n",
    "# plot cost function\n",
    "plt.plot(J)\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8729281767955801\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_ground_truth,y_pred):\n",
    "    ### YOUR CODE GOES  HERE ###\n",
    "    return np.mean(y_ground_truth == y_pred)\n",
    "\n",
    "pred_one_max = compute_forward(x_test, theta_matrices)[1]\n",
    "pred = np.argmax(pred_one_max, axis=1)\n",
    "# Your prediction would be an one-hot vector, for each test sample, select the one with the highest probablity to assign the class\n",
    "accuracy = compute_accuracy(y_test, pred)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
