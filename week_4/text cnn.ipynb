{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this code only once\n",
    "!wget https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/reviews.txt -P data/\n",
    "!wget https://raw.githubusercontent.com/cezannec/CNN_Text_Classification/master/data/labels.txt -P data/\n",
    "!wget https://github.com/bekou/multihead_joint_entity_relation_extraction/raw/master/data/CoNLL04/vecs.lc.over100freq.zip -P data/\n",
    "!unzip data/vecs.lc.over100freq.zip -d data/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print the first 1000 characters from the text--note this is not yet \n",
    "ready for classification, we should split it to sentences first\n",
    "\"\"\"\n",
    "###TODO print the first 1000 characters of the reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print the first 20 characters from the labels--note this is not yet \n",
    "ready for classification, we should split it to sentence-level labels first\n",
    "\"\"\"\n",
    "###TODO print the first 20 characters of the labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\"\"\"First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words.\"\"\"\n",
    "\n",
    "# get rid of punctuation\n",
    "reviews = reviews.lower() # lowercase, standardize\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = ##TODO split the text (i.e., all_text) using the newline character \\n to create a list of the document sentences. Hint! size of the list should be 25001\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of all words\n",
    "all_words = all_text.split() # this is the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_split[0] # this is what the data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews_split) # this is the size of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Labels\n",
    "\n",
    "The review labels are \"positive\" or \"negative\". \n",
    "Split them with the newline character to have the same size as the reviews_split list. \n",
    "Then transform them into 0 or 1 to feed them to the neural network. \n",
    "1 in the case that the label is positive, 0 in the case that it is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive, 0=negative label conversion\n",
    "labels_split = ##TODO split the labels using the newline character \\n to create a list of labels. Hint! size of the list should be 25001\n",
    "encoded_labels = ##TODO encode the labels_split into 0-1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_split[0] # this is what the labels look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_split) # this is the size of the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Build a dictionary that maps indices to review lengths\n",
    "counts = Counter(all_words)\n",
    "# outlier review stats\n",
    "# counting words in each review\n",
    "review_lens = Counter([len(x.split()) for x in reviews_split])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of extremely long or short reviews; the outliers\n",
    "\n",
    "print('Number of reviews before removing outliers: ', len(reviews_split))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_split) if len(review.split()) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_split = [reviews_split[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pre-Trained Embedding Layer\n",
    "\n",
    "Next, we will tokenize the reviews; turn list of words that make up a given review into a list of tokenized integers that represent those words. Typically, this is done by creating a dictionary that maps each unique word in a vocabulary to a specific integer value.\n",
    "\n",
    "In this example, we will use a mapping that already exists, in a pre-trained embedding layer. Below, we are loading a pre-trained embeddings matrix.\n",
    "\n",
    "You can download the matrix and explore it your self (from the link at the beginning of the exercise). You can open it as a text document via e.g., notepad++. It contains 412128 tokens with the corresponding pretrained vector for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Word2Vec loading capabilities \n",
    "from gensim.models import KeyedVectors\n",
    "# Creating the model\n",
    "embed_lookup = KeyedVectors.load_word2vec_format('data/vecs.lc.over100freq.txt', \n",
    "                                                 binary=False,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You can think of an embedding layer as a lookup table, where the rows are indexed by word token and the \n",
    "columns hold the embedding values. For example, row 4 is the embedding vector for the ''the'' word that maps to the integer value 4.\n",
    "See the embeddings matrix for that or the pretrained_words list below.\n",
    "In the cells below, we are storing the words in the pre-trained vocabulary, and printing out the size of the \n",
    "vocabulary and word embeddings. The embedding dimension from the pre-trained model is 50.\"\"\"\n",
    "\n",
    "# store pretrained vocab\n",
    "pretrained_words = []\n",
    "for word in embed_lookup.vocab:\n",
    "    pretrained_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = 4\n",
    "\n",
    "# get word/embedding in that row\n",
    "word = ## TODO print the word with the idx 4\n",
    "embedding = ## TODO print the corresponding embedding of that word\n",
    "\n",
    "# vocab and embedding info\n",
    "print(\"Size of Vocab: {}\\n\".format(len(pretrained_words)))\n",
    "print('Word in vocab: {}\\n'.format(word))\n",
    "print('Length of embedding: {}\\n'.format(len(embedding)))\n",
    "print('Associated embedding: \\n', embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few common words -- Note that in the word embeddings matrix the words are sorted by frequency of appearance in the documents used for pre-training the word embeddings\n",
    "for i in range(3,8):\n",
    "    print(pretrained_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize reviews\n",
    "\n",
    "The pre-trained embedding layer already has tokens associated with each word in the dictionary. We want to use that same mapping to tokenize all the reviews in the movie review corpus. We will encode any unknown words (words that appear in the reviews but not in the pre-trained vocabulary) as the unknown token (unk), 9; This appears at the 9th position of the embedding matrix. All words that appear in the vocabulary, we will encode them with the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reviews to tokens\n",
    "def tokenize_all_reviews(embed_lookup, reviews_split):\n",
    "    # split each review into a list of words\n",
    "    reviews_words = [review.split() for review in reviews_split]\n",
    "\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews_words:\n",
    "        ints = []\n",
    "        for word in review:\n",
    "            try:\n",
    "                idx = ## TODO add the code based on the description \n",
    "            except: \n",
    "                idx = ## TODO add the code based on the description\n",
    "                \n",
    "            ints.append(idx)\n",
    "        tokenized_reviews.append(ints)\n",
    "    \n",
    "    return tokenized_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = tokenize_all_reviews(embed_lookup, reviews_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code and printing a tokenized review for the first document\n",
    "print(tokenized_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long reviews, we should have the same length for all reviews in the neural network\n",
    "For that, we will pad all reviews shorter than a specific length with a number. For us, it will be 1 because this is \n",
    "the PADDING id in the embeddings matrix (validate that in the vocabulary). \n",
    "For all reviews that are longer than this specific length, we will cut them.\n",
    "\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input, tokenized review is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The padded sequence should be: \n",
    "\n",
    "```\n",
    "[1, 1, 1, 1 1, 1, 1, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tokenized_reviews, seq_length):\n",
    "    ''' Return features of tokenized_reviews, where each review is padded either with 1's until the size of the seq_length\n",
    "        or is cut to the input seq_length.\n",
    "    '''\n",
    "    features = ##TODO calculate the features matrix -- it should be of size number of reviews x seq_length\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(tokenized_reviews, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(tokenized_reviews), \"Features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# shuffling and batching data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Network with PyTorch\n",
    "\n",
    "The complete model consists of a few layers:\n",
    "\n",
    "**1. An [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)**\n",
    "* This converts our word tokens (integers) into embedded vectors of a specific size.\n",
    "* In this case, the vectors/weights of this layer will come from the **pretrained** lookup table defined above. \n",
    "\n",
    "**2. A few [convolutional layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)**\n",
    "* These are defined by an input size, number of filters/feature maps to output, and a kernel size.\n",
    "* The output of these layers will go through a ReLu activation function and pooling layer in the `forward` function.\n",
    "\n",
    "**3. A fully-connected, output layer**\n",
    "* This maps the convolutional layer outputs to a desired output_size (1 sentiment class).\n",
    "\n",
    "**4. A sigmoid activation layer**\n",
    "* This turns the output logit into a value 0-1; a class score.\n",
    "\n",
    "There is also a dropout layer, which will prevent overfitting, placed between the convolutional outputs and the final, fully-connected layer.\n",
    "\n",
    "See the original paper, [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf).\n",
    "\n",
    "### The Embedding Layer\n",
    "\n",
    "The embedding layer comes from our pre-trained `embed_lookup` model. By default, the weights of this layer are set to the vectors from the pre-trained model and frozen, so it will just be used as a lookup table. You could train your own embedding layer here, but it will speed up the training process to use a pre-trained model.\n",
    "\n",
    "### The Convolutional Layer(s)\n",
    "\n",
    "So why use CNNs on text? In the same way that a 3x3 filter can look over a patch of an image, a 1x2 filter can look over a 2 sequential words in a piece of text, i.e. a bi-gram. In this CNN model we will use multiple filters of different sizes which will look at the bi-grams (two words, a 1x2 filter), tri-grams (three words, a 1x3 filter) and/or n-grams (a 1x$n$ filter) within the text.\n",
    "\n",
    "The intuition here is that the appearance of certain bi-grams, tri-grams and n-grams within the review will be a good indication of the final sentiment.\n",
    "\n",
    "We can then use a filter that is [n x emb_dim]. This will cover $n$ sequential words entirely, as their width will be emb_dim dimensions.\n",
    "\n",
    "The kernel_sizes would (3, 50), (4, 50), and (5, 50); to look at 3-, 4-, and 5- sequences of word embeddings at a time (50 is the size of the word embeddings). Each of these three layers will produce 100 filtered outputs, where 100 is the number of filters. \n",
    "\n",
    "The kernels only move in one dimension: down to a sequence of word embeddings. In other words, these kernels move along a sequence of words, in time!\n",
    "\n",
    "### Maxpooling Layers\n",
    "\n",
    "In the `forward` function, we apply a ReLu activation to the outputs of all convolutional layers and a maxpooling layer over the input sequence dimension. The maxpooling layer will get us an indication of whether some high-level text feature was found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The embedding layer + CNN model that will be used to perform sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n",
    "                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentCNN, self).__init__()\n",
    "\n",
    "        # set class vars\n",
    "        self.num_filters = num_filters\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # set weights to pre-trained\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n",
    "        # (optional) freeze embedding weights\n",
    "        if freeze_embeddings:\n",
    "            self.embedding.requires_grad = False\n",
    "        \n",
    "        # 2. convolutional layers             \n",
    "        \n",
    "        self.conv_0 = ##TODO create a convolutional neural network layer using Conv2d that takes into account 3-grams\n",
    "        \n",
    "        self.conv_1 = ##TODO create a convolutional neural network layer using Conv2d that takes into account 4-grams\n",
    "        \n",
    "        self.conv_2 = ##TODO create a convolutional neural network layer using Conv2d that takes into account 5-grams\n",
    "        \n",
    "        self.fc = ##TODO create a fully connected layer that projects from the concatanated representations to the number of outputs\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.sigmoid = ##TODO turns scores to probabilities \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines how a batch of inputs, x, passes through the model layers.\n",
    "        Returns a single, sigmoid-activated class score as output.\n",
    "        \"\"\"\n",
    "        # embedded vectors\n",
    "        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n",
    "        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n",
    "        embeds = embeds.unsqueeze(1)\n",
    "                \n",
    "        #embedded = [batch size, 1, sent len, emb dim]  \n",
    "        \n",
    "        conved_0 = ##TODO apply a relu + the convolution on the embedded representation of the text for 3-grams\n",
    "        conved_1 = ##TODO apply a relu + the convolution on the embedded representation of the text for 4-grams\n",
    "        conved_2 = ##TODO apply a relu + the convolution on the embedded representation of the text for 5-grams\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = ##TODO apply max pooling using max_pool1d for 3-grams\n",
    "        pooled_1 = ##TODO apply max pooling using max_pool1d for 4-grams\n",
    "        pooled_2 = ##TODO apply max pooling using max_pool1d for 5-grams\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))    \n",
    "        \n",
    "        logit = self.fc(cat)        \n",
    "        \n",
    "        return self.sig(logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we will instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `num_filters`: Number of filters that each convolutional layer produces as output.\n",
    "* `filter_sizes`: A list of kernel sizes; one convolutional layer will be created for each kernel size.\n",
    "\n",
    "Any parameters I did not list, are left as the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "\n",
    "vocab_size = len(pretrained_words)\n",
    "output_size = 1 # binary class (1 or 0)\n",
    "embedding_dim = len(embed_lookup[pretrained_words[0]]) # 50-dim vectors\n",
    "num_filters = 100\n",
    "kernel_sizes = [3, 4, 5]\n",
    "\n",
    "net = SentimentCNN(embed_lookup, vocab_size, output_size, embedding_dim,\n",
    "                   num_filters, kernel_sizes)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(net, train_loader, epochs, print_every=100):\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0 # for printing\n",
    "    \n",
    "    # train for some number of epochs\n",
    "    net.train()\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            output = net(inputs)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    output = net(inputs)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "epochs = 2 # this is approx where I noticed the validation loss stop decreasing\n",
    "print_every = 100\n",
    "train(net, train_loader, epochs, print_every=print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    \n",
    "    output = ## TODO get predicted outputs\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = ##TODO transform the output probabilities to the predicted class. Hint! Rounding or use a threshold of 0.5\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
